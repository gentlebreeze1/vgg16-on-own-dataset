{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangyong/anaconda/envs/tensorflow2/lib/python2.7/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/Users/zhangyong/anaconda/envs/tensorflow2/lib/python2.7/site-packages/matplotlib/mpl-data/matplotlibrc\", line #42\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "import numpy as np  \n",
    " \n",
    "import datasetmake\n",
    "#定义网络参数  \n",
    "learning_rate = 0.001  \n",
    "display_step = 5  \n",
    "epochs = 10  \n",
    "keep_prob = 0.5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot:0\", shape=(2, 2), dtype=float32) Tensor(\"shuffle_batch:0\", shape=(2, 224, 224, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_op(input_op, name, kh, kw, n_out, dh, dw):  \n",
    "    input_op = tf.convert_to_tensor(input_op)  \n",
    "    n_in = input_op.get_shape()[-1].value  \n",
    "    with tf.name_scope(name) as scope:  \n",
    "        kernel = tf.get_variable(scope+\"w\",  \n",
    "                                shape = [kh, kw, n_in, n_out],  \n",
    "                                dtype = tf.float32,  \n",
    "                                initializer = tf.contrib.layers.xavier_initializer_conv2d())  \n",
    "        conv = tf.nn.conv2d(input_op, kernel, (1, dh, dw, 1), padding = 'SAME')  \n",
    "        bias_init_val = tf.constant(0.0, shape = [n_out], dtype = tf.float32)  \n",
    "        biases = tf.Variable(bias_init_val, trainable = True, name = 'b')  \n",
    "        z = tf.nn.bias_add(conv, biases)  \n",
    "        activation = tf.nn.relu(z, name = scope)  \n",
    "        return activation  \n",
    "  \n",
    "#定义全连接操作  \n",
    "def fc_op(input_op, name, n_out):  \n",
    "    n_in = input_op.get_shape()[-1].value  \n",
    "    with tf.name_scope(name) as scope:  \n",
    "        kernel = tf.get_variable(scope+'w',  \n",
    "                                shape = [n_in, n_out],  \n",
    "                                dtype = tf.float32,  \n",
    "                                initializer = tf.contrib.layers.xavier_initializer())  \n",
    "        biases = tf.Variable(tf.constant(0.1, shape = [n_out], dtype = tf.float32), name = 'b')  \n",
    "        # tf.nn.relu_layer对输入变量input_op与kernel做矩阵乘法加上bias，再做RELU非线性变换得到activation  \n",
    "        activation = tf.nn.relu_layer(input_op, kernel, biases, name = scope)   \n",
    "        return activation  \n",
    "      \n",
    "#定义池化层  \n",
    "def mpool_op(input_op, name, kh, kw, dh, dw):  \n",
    "    return  tf.nn.max_pool(input_op,  \n",
    "                           ksize = [1, kh, kw, 1],  \n",
    "                           strides = [1, dh, dw, 1],  \n",
    "                           padding = 'SAME',  \n",
    "                           name = name)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_op(input_op, keep_prob):  \n",
    "    # block 1 -- outputs 112x112x64  \n",
    "    conv1_1 = conv_op(input_op, name=\"conv1_1\", kh=3, kw=3, n_out=64, dh=1, dw=1)  \n",
    "    conv1_2 = conv_op(conv1_1,  name=\"conv1_2\", kh=3, kw=3, n_out=64, dh=1, dw=1)  \n",
    "    pool1 = mpool_op(conv1_2,   name=\"pool1\",   kh=2, kw=2, dw=2, dh=2)  \n",
    "  \n",
    "    # block 2 -- outputs 56x56x128  \n",
    "    conv2_1 = conv_op(pool1,    name=\"conv2_1\", kh=3, kw=3, n_out=128, dh=1, dw=1)  \n",
    "    conv2_2 = conv_op(conv2_1,  name=\"conv2_2\", kh=3, kw=3, n_out=128, dh=1, dw=1)  \n",
    "    pool2 = mpool_op(conv2_2,   name=\"pool2\",   kh=2, kw=2, dh=2, dw=2)  \n",
    "  \n",
    "    # # block 3 -- outputs 28x28x256  \n",
    "    conv3_1 = conv_op(pool2,    name=\"conv3_1\", kh=3, kw=3, n_out=256, dh=1, dw=1)  \n",
    "    conv3_2 = conv_op(conv3_1,  name=\"conv3_2\", kh=3, kw=3, n_out=256, dh=1, dw=1)  \n",
    "    conv3_3 = conv_op(conv3_2,  name=\"conv3_3\", kh=3, kw=3, n_out=256, dh=1, dw=1)      \n",
    "    pool3 = mpool_op(conv3_3,   name=\"pool3\",   kh=2, kw=2, dh=2, dw=2)  \n",
    "  \n",
    "    # block 4 -- outputs 14x14x512  \n",
    "    conv4_1 = conv_op(pool3,    name=\"conv4_1\", kh=3, kw=3, n_out=512, dh=1, dw=1)  \n",
    "    conv4_2 = conv_op(conv4_1,  name=\"conv4_2\", kh=3, kw=3, n_out=512, dh=1, dw=1)  \n",
    "    conv4_3 = conv_op(conv4_2,  name=\"conv4_3\", kh=3, kw=3, n_out=512, dh=1, dw=1)  \n",
    "    pool4 = mpool_op(conv4_3,   name=\"pool4\",   kh=2, kw=2, dh=2, dw=2)  \n",
    "    conv5_1 = conv_op(pool4,    name=\"conv5_1\", kh=3, kw=3, n_out=512, dh=1, dw=1)  \n",
    "    conv5_2 = conv_op(conv5_1,  name=\"conv5_2\", kh=3, kw=3, n_out=512, dh=1, dw=1)  \n",
    "    conv5_3 = conv_op(conv5_2,  name=\"conv5_3\", kh=3, kw=3, n_out=512, dh=1, dw=1)  \n",
    "    pool5 = mpool_op(conv5_3,   name=\"pool5\",   kh=2, kw=2, dw=2, dh=2)  \n",
    "  \n",
    "    # flatten  \n",
    "    shp = pool5.get_shape()  \n",
    "    flattened_shape = shp[1].value * shp[2].value * shp[3].value  \n",
    "    resh1 = tf.reshape(pool5, [-1, flattened_shape], name=\"resh1\")  \n",
    "  \n",
    "    # fully connected  \n",
    "    fc6 = fc_op(resh1, name=\"fc6\", n_out=4096)  \n",
    "    fc6_drop = tf.nn.dropout(fc6, keep_prob, name=\"fc6_drop\")  \n",
    "  \n",
    "    fc7 = fc_op(fc6_drop, name=\"fc7\", n_out=4096)  \n",
    "    fc7_drop = tf.nn.dropout(fc7, keep_prob, name=\"fc7_drop\")  \n",
    "  \n",
    "    logits = fc_op(fc7_drop, name=\"fc8\", n_out=2)  \n",
    "    return logits  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(logits,labels):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))  \n",
    "  \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)  \n",
    "  \n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(labels,1))  \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))  \n",
    "    return optimizer, cost, accuracy  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "9.202 0.0\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "5.87019 0.0\n",
      "training finish!\n",
      "Test acc = 0.4\n",
      "Test Finish!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "  \n",
    "   \n",
    "     train_filename = \"train.tfrecords\"  \n",
    "     test_filename = \"test.tfrecords\"  \n",
    "     image_batch, label_batch = datasetmake.createBatch(filename = train_filename, batchsize=2)  \n",
    "     test_image, test_label = datasetmake.createBatch(filename = test_filename, batchsize=20)  \n",
    "     pred = inference_op(input_op = image_batch, keep_prob = keep_prob)  \n",
    "     test_pred = inference_op(input_op = test_image, keep_prob = keep_prob)  \n",
    "     optimizer, cost, accuracy = train(logits = pred, labels = label_batch)  \n",
    "     test_optimizer, test_cost, test_acc = train(logits = test_pred, labels = test_label)  \n",
    "     initop = tf.group(tf.global_variables_initializer(),tf.local_variables_initializer())  \n",
    "     with tf.Session() as sess:     \n",
    "        sess.run(initop)  \n",
    "        coord = tf.train.Coordinator()  \n",
    "        threads = tf.train.start_queue_runners(sess = sess, coord = coord)          \n",
    "        step = 0  \n",
    "        while step < epochs:  \n",
    "            step += 1  \n",
    "            print step  \n",
    "            _, loss, acc = sess.run([optimizer,cost,accuracy])  \n",
    "            if step % display_step ==0:    \n",
    "                print loss,acc  \n",
    "        print \"training finish!\"  \n",
    "        _, testLoss, testAcc = sess.run([test_optimizer,test_cost,test_acc])  \n",
    "        print \"Test acc = \"+ str(testAcc)  \n",
    "        print \"Test Finish!\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
